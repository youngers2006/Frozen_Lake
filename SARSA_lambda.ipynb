{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03181fec",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9fdcee",
   "metadata": {},
   "source": [
    "Create env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeddb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf80e87",
   "metadata": {},
   "source": [
    "Hyperparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "episodes = 100\n",
    "epsilon = 1.0\n",
    "ep_decay = 0.9\n",
    "gamma = 0.99\n",
    "lambda_ = 0.9\n",
    "Learn_Rate = 0.05\n",
    "\n",
    "key = jax.random.PRNGKey(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d0b379",
   "metadata": {},
   "source": [
    "Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(Q, state, epsilon, key):\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    rand = jax.random.uniform(key=subkey, shape=(1,), minval=0.0, maxval=1.0)\n",
    "\n",
    "    if rand >= epsilon:\n",
    "        action = int(jnp.argmax(Q[state,:]))\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a7f4c",
   "metadata": {},
   "source": [
    "Define Q function and Eligibility trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74574815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Q(s,a) for all s,a pairs\n",
    "# Q is tabular and there are 16 states and 4 actions in each state\n",
    "Q = jax.random.uniform(key=key,shape=(num_states,num_actions), minval=0.0, maxval=0.001)\n",
    "\n",
    "E = jnp.zeros(shape=(num_states,num_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaba082",
   "metadata": {},
   "source": [
    "Run train sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = []\n",
    "\n",
    "for episode in tqdm(range(episodes), leave=False):\n",
    "    E = jnp.zeros_like(E)\n",
    "    initial_observation, info = env.reset(seed=seed)\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    state = initial_observation\n",
    "    action = policy(Q, state, epsilon, key)\n",
    "    total_reward = 0\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        state_prime = observation\n",
    "        action_prime = policy(Q, state_prime, epsilon, key)\n",
    "\n",
    "        TD_error = reward + gamma * Q[state_prime,action_prime] - Q[state,action]\n",
    "        Esa = E[state,action]\n",
    "        E = E.at[state,action].set(Esa + 1)\n",
    "\n",
    "        for s in range(num_states):\n",
    "            for a in range(num_actions):\n",
    "                Qsa = Q[s,a]\n",
    "                Esa = E[s,a]\n",
    "                Q = Q.at[s,a].set(Qsa + Learn_Rate * TD_error * Esa)\n",
    "                E = E.at[s,a].set(gamma * lambda_ * Esa)\n",
    "\n",
    "        state = state_prime\n",
    "        action = action_prime\n",
    "\n",
    "    reward_list.append(total_reward)\n",
    "    epsilon = epsilon * ep_decay\n",
    "    seed = seed + episode\n",
    "\n",
    "plt.plot(reward_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef9360",
   "metadata": {},
   "source": [
    "Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2897d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list_test = []\n",
    "\n",
    "rewinds = 20\n",
    "\n",
    "for episode in range(rewinds):\n",
    "    initial_observation, info = env.reset(seed=seed)\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    state = initial_observation\n",
    "    action = policy(Q, state, epsilon, key)\n",
    "    total_reward = 0\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        state_prime = observation\n",
    "        action_prime = policy(Q, state_prime, epsilon, key)\n",
    "\n",
    "        state = state_prime\n",
    "        action = action_prime\n",
    "\n",
    "    reward_list_test.append(total_reward)\n",
    "\n",
    "plt.plot(reward_list_test)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
